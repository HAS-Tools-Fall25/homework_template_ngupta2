{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff4a436-7732-4571-bd9d-e6d3d778d661",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `xarray` exercises pt 2: ENSO Analysis\n",
    "\n",
    "Welcome, this is a the second part on xarray. In it, we will use data from  [Pangeo-Forge](https://pangeo-forge.org/).\n",
    "[Pangeo](https://pangeo.io/) is a community platform for big data geoscience. We will use cloud-hosted datasets of global precipitation and sea surface temperatures to investigate the role of ENSO in monsoon precipitation in both Asia and the Southwest US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb4c6c-3b13-4d7d-8f84-b3b99887a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8adaa-5d7b-4d70-9f06-b9ac0671d5b8",
   "metadata": {},
   "source": [
    "# Pulling in our data\n",
    "We will pull in precipitation data from [NOAA Daily Global Precipitation data](https://www.ncei.noaa.gov/products/climate-data-records/precipitation-gpcp-daily) and sea surface temperature data from the [Hadley Centre Global Sea Ice and Sea Surface Temperature](https://climatedataguide.ucar.edu/climate-data/sst-data-hadisst-v11). \n",
    "\n",
    "To access the data all we need to do is put in the url to the dataset on Pangeo Forge. That can just get passed into xarray and we are good to go. Note this dataset is about 3.0 GB, and the file opens basically instantly. This is because xarray is lazy and only opens the metadata. Data will only be pulled in over the network when we actually do something with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec688d9-40f7-49b4-9053-e8ee75e96187",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm_file = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/gpcp-feedstock/gpcp.zarr'\n",
    "gpm = xr.open_dataset(gpm_file, engine='zarr', chunks={})\n",
    "gpm = gpm['precip']\n",
    "\n",
    "sst_file = 'https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/HadISST-feedstock/hadisst.zarr'\n",
    "sst = xr.open_dataset(sst_file, engine='zarr', chunks={})\n",
    "sst = sst['sst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5a54c-ab12-47c6-acc5-51fcaf8a9a00",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 1: Plotting seasonal average precipitations\n",
    "\n",
    "## Part 1\n",
    "Let's begin by familiarizing ourselves with the precipitation data. To do so, let's compute the seasonal average precipitation across the globe for the period of record (aka all times in the dataset).\n",
    "\n",
    "Your goal here is to group the `gpm` data by the `gpm['time'].dt.season` and compute the mean over the `time` dimension. The resulting dimensions of the new dataset should be `(season, latitude, longitude)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76fdc6-4830-41ba-86c2-9dceb4d55a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_precip = gpm # TODO: this part yours...\n",
    "seasonal_precip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddd52a-29dc-4c06-9b74-51d8a28e046f",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now that you've got your dataset aggregated, let's make a plot of what's in it. For this one, we'll use `xarray`'s built in plotting capability to make things easy. Call `.plot` on the `seasonal_precip` DataArray. Use the following arguments to make your plot nicer:\n",
    " * `col='season'`: Each season will be as separate column\n",
    " * `col_wrap=2` : Wrap over to a new row after 2 columns. This makes the plot a 2x2 grid of subplots\n",
    " * `cmap='turbo'`: Use the \"turbo\" colormap.\n",
    " \n",
    "Note 1: This will take a minute or two to complete, remember we're actually working with gigabytes of data.\n",
    "\n",
    "Note 2: The `with ProgressBar()` outer bit makes it so that we track how var in the computation things are. This should slowly tick up so that you can ensure things are happening. This is special to `dask` and cloud datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd16550-9260-42c0-aae9-7e18183b9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    seasonal_precip # TODO: this part yours..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bfbaaf-2370-414f-a701-e4152081c495",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "\n",
    "Hey! That looks like garbage!? Often times you'll need to do some data cleanup before you can run your analysis, as \"bad values\" can ruin everything like we've seen above. To fix things, let's create a `mask` variable which we will use to filter out really large values. Specifically, we want to select data when `gpm['precip'] < 1000.0` and where `gpm['precip'] > 0`. You can join logical statements with the `&` operator, just like using `+` to add two arithmetic operations. Your only task here is to create the mask variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008d879-0be3-4207-9db9-a6847fb11ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    low_condition = None\n",
    "    high_condition = None\n",
    "    mask = low_condition & high_condition\n",
    "    seasonal_precip = (gpm.where(mask)\n",
    "                          .groupby(gpm['time'].dt.season)\n",
    "                          .mean(dim='time')\n",
    "                          .compute())\n",
    "\n",
    "seasonal_precip.plot(col='season', col_wrap=2, cmap='turbo', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1ac10-4f1a-4073-a1f9-00559bc5b89b",
   "metadata": {},
   "source": [
    "## Part 4:\n",
    "The plot from part 3 should now be a lot more interesting. Describe briefly what you're seeing below.\n",
    "\n",
    "\n",
    "#### Your response below:\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0f290-78fc-4c75-ab0d-e8af09cd6e02",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 2: Calculating an ENSO index\n",
    "\n",
    "We don't have nearly enough time to get into the science, but briefly the El-Ni√±o Southern Oscillation (ENSO) is a phenomena of periodic changes in the sea surface temperature and wind patterns in the eastern equatorial Pacific Ocean. This oscillation has strong impacts on many aspects of the climate system, including monsoonal precipitation patterns. The figure below shows a timeseries of sea surface temperature anomalies for this region. The dotted points are the actual data, black line is a smoothed average, and the shaded blue/red regions show prolonged deviations from the mean. We will first re-create a similar plot to this.\n",
    "\n",
    "![image.png](https://upload.wikimedia.org/wikipedia/commons/0/02/Soi.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f43a2b-3e04-4214-ab44-bcd508d87280",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get from the global SST dataset to a timeseries of anomalies we will need 4 parts. \n",
    "\n",
    "  1) First, select out the latitude range of $(+5, -5)$ and the longitude range of $(-170, -150)$ degrees. Note the sign ordering.\n",
    "  2) Take the mean of this resulting `nino_ext` DataArray over the `['latitude', 'longitude']` dimensions and assign this to `nino_sst`.\n",
    "  3) Also calculating the mean over the `'time'` dimension from `nino_sst`. Assign this to `nino_mean`\n",
    "  4) Finally, compute the `nino_anomalies` by subtracting the `nino_mean` from `nino_sst`\n",
    "  \n",
    "### Part 1:\n",
    "First, select out the latitude range of $(+5, -5)$ and the longitude range of $(-170, -150)$ degrees. Note the sign ordering.\n",
    "\n",
    "The resulting DataArray should have dimensions:     \n",
    " * latitude: 10 \n",
    " * longitude: 20\n",
    " * time: 1829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b338a-e0e0-477b-abb2-374a2ba34861",
   "metadata": {},
   "outputs": [],
   "source": [
    "nino_lat = None\n",
    "nino_lon = None\n",
    "nino_extent = sst.sel(latitude=nino_lat, longitude=nino_lon)\n",
    "nino_extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b33872-868e-479a-9b7f-cce059131afb",
   "metadata": {},
   "source": [
    "### Part 2:\n",
    "Take the mean of this resulting `nino_ext` DataArray over the `['latitude', 'longitude']` dimensions and assign this to `nino_sst`.\n",
    "\n",
    "The resulting DataArray should only have a time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a129196-e096-48fe-9f26-48651f027e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "nino_sst = nino_extent # TODO: Your code to complete this...\n",
    "nino_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853809d2-50c4-4341-9a04-3a5392da1755",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 3:\n",
    "Calculate the mean over the `'time'` dimension from `nino_sst`. Assign this to `nino_mean`\n",
    "\n",
    "The resulting DataArray should have no dimensions, and a `shape` of `()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19b094-0041-48c2-83a7-234d963bc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "nino_mean = nino_sst # TODO: Your code to complete this\n",
    "nino_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07973fe2-6eab-4f4f-b73c-d5fb5f02f807",
   "metadata": {},
   "source": [
    "###  Part 4:\n",
    "\n",
    "Finally, compute the `nino_anomalies` by subtracting the `nino_mean` from `nino_sst`\n",
    "\n",
    "The resulting DataArray should have dimensions of time only. Values should be both positive and negative. You can even check to ensure that the mean is close to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab480fc-9b3d-4f7c-bae1-ec9c85427b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    nino_anomalies = None # TODO: your code to complete\n",
    "\n",
    "nino_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84189c76-b8ca-4854-bbed-85a0156a3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "nino_anomalies.mean().round(2) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53a034-982b-4ecc-91bf-a66903a49bea",
   "metadata": {},
   "source": [
    "## Putting step 2 together\n",
    "\n",
    "If you've made it this far you should be able to run the following code cell to create a plot like the one shown at the beginning of Step 2. The plot won't look exactly the same because we are using a different dataset, different region of aggregation, and different method of plotting the smoothed timeseries. But, you should be able to see a similarity in the ebb-and-flow of the ENSO signature that we're plotting below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6af77-9ebe-4121-9513-b8cd6b4f64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(18, 5))\n",
    "nino_anomalies.plot(marker='.', linestyle='', color='lightgrey', label='Data points')\n",
    "nino_anomalies.rolling(time=12, center=True).mean().plot(label='Smoothed')\n",
    "ax.axhline(0, color='black')\n",
    "plt.ylabel(r'SST Anomalies ($^{\\circ} C$)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe6b2e-de80-49cb-aba8-5a50ba23df5f",
   "metadata": {},
   "source": [
    "## Step 3: Merging the SST and Precipitation datasets\n",
    "\n",
    "One of the fundamental skills for doing science on large datasets is being able to match up their temporal/spatial resolutions to facilitate analysis. Our two datasets have different time extents, time resolutions, and spatial resolutions. To be able to merge them together we'll need to either reduce some of the data or match the spatiotemporal variablity. \n",
    "\n",
    "We've actually already taken care of one pieces of this by calculating the `nino_anomalies` in the previous step, which now simply has a `time` dimension at the monthly timescale. But there are still some loose ends. To clean them up you'll need to complete 3 parts:\n",
    "\n",
    "1) Match up the time extents for the `gpm` and `nino_anomalies` datasets.\n",
    "2) Match up the time stamps for the `gpm` and `nino_anomalies` datasets.\n",
    "3) Re-apply the masking technique from earlier to filter out bad precipitation data.\n",
    "\n",
    "### Part 1:\n",
    "\n",
    "Find the first and last times in both the `gpm` and `nino_anomalies` datasets. These can be computed by calling the `min` and `max` functions on the \"time\" dimension of each dataset, respectively. This cell should print out a single time slice with times starting in 1996 and ending in 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05b82b-46d9-4112-82f5-58f4d869020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_gpm = None\n",
    "last_gpm = None\n",
    "\n",
    "first_nino = None\n",
    "last_nino = None\n",
    "\n",
    "first_time = max(first_gpm, first_nino)\n",
    "last_time = min(last_gpm, last_nino)\n",
    "time_slice = slice(first_time, last_time)\n",
    "time_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f39a79-16ff-455e-a883-d6871bcfb5a4",
   "metadata": {},
   "source": [
    "### Part 2:\n",
    "\n",
    "Next, we'll need to align the timestamps so that they fit together. To do so, I've set you up to select out the correct time slice from the previous part but you'll need to do the `resample` to a monthly mean. This can be accomplished by calling the `resample` function with an argument which is a dictionary where the key is `'time'` and the `value` is `'1M'` (which means \"one month\"). Finally, after the call to resample you can compute the mean with the `mean` function. These `monthly_nino` and `monthly_gpm` variables should be able to be placed into the same dataset which has dims of `time`, `latitude`, and `longitude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06bab2-c41c-4324-a793-8353f76345da",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_nino = nino_anomalies.sel(time=time_slice)\n",
    "monthly_gpm = gpm.sel(time=time_slice)\n",
    "\n",
    "monthly_nino = monthly_nino.resample # TODO: your code here...\n",
    "monthly_gpm = monthly_gpm.resample  # TODO: your code here...\n",
    "\n",
    "# Putting things into a new dataset with shared coordinates\n",
    "ds = xr.Dataset()\n",
    "ds['nino_anomalies'] = monthly_nino\n",
    "\n",
    "ds['precip'] = monthly_gpm\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b045acf-ef75-4cb4-9f51-d1bd77c01127",
   "metadata": {},
   "source": [
    "### Part 3:\n",
    "Finally, you'll need to mask out the same precipitation data as you did in Step 1 Part 3. This should yield a plot which has some meaningful patterns. This might take some time to complete, so you should make sure that the first code cell completes with no errors before attempting to run the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe000b11-9d56-418f-a008-532007e4c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = None # TODO: your code here\n",
    "ds['precip'] = ds['precip'].where(mask, other=np.nan)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0764b43-3da9-4c28-ab9e-3805f50b62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    ds['precip'].mean(dim='time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e774a4-e7e7-4501-965a-b31c0bdbb9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 4: How does the ENSO index correspond to precipitation?\n",
    "\n",
    "We're finally to the end of this thing! Now that you've got your precipitation data and ENSO signature on the same timestamp and filtered only to meaningful data it's time to look at the correlations. We'll first look at this for a particular region around Indonesia and then for the global context.\n",
    "\n",
    "This will happen in 4 parts. \n",
    "  1) First, select out the latitude range $(0, 10)$ and longitude with range $(100, 130)$, and take the spatial mean for the dataset `ds`.\n",
    "  2) Once calculated, plot the two timeseries for the ENSO anomaly and precipitation on separate subplots.\n",
    "  3) Make a scatter plot of the same variables. \n",
    "  4) Finally, calculate the global correlation between the ENSO signature and precipitation timeseries and plot the correlation as a map.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Here you should be familiar with using either the `range` or `slice` notation to set up the `sea_monsoon_lat` and `sea_monsoon_lon` variables. Just setting those up should be all you need to do, I've already put together the reduction to a spatial mean together for you inside of the `with ProgressBar()` clause. This will probably take a minute or two to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fde9fc-4c07-477e-9345-a95f125b5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_monsoon_lat = None\n",
    "sea_monsoon_lon = None\n",
    "with ProgressBar():\n",
    "    sea_monsoon_ppt = ds.sel(\n",
    "        latitude=sea_monsoon_lat, \n",
    "        longitude=sea_monsoon_lon\n",
    "    ).mean(dim=['latitude', 'longitude']).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c7e02-3918-4e27-965f-1ea79bdc3826",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "With the data calculated you just have to put the right timeseries on the subplots. This should be reminiscent of the previous assignment where you had to make similar subplots. Just make sure that you're plotting both the `nino_anomalies` and `precip` variables on separate subplots and you should be good to go.\n",
    "\n",
    "When looking at the resulting plots, do you see any correlation between the two? Feel free to write your observations below, though it's not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aed813-5769-476a-805b-6754402fe5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, sharex=True)\n",
    "# TODO: Your plotting commands here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc14f2-b863-45d2-9f4d-73d7b702ab04",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "To confirm what you suspected from the previous plot let's also do a scatter plot of it all. All you need to do here is fill in the variable names for `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30af3c1-6fda-4a22-a721-0a210b4054fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_ds.plot.scatter(\n",
    "    # TODO: something should go in here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686b38a-2919-4972-89cb-b499143ab337",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "Finally, let's bring out the big moves and compute the correlation between these SST anomalies and precipitation amounts. To do so, use the `xr.corr` method like we did in the last assignment and plot the resulting correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fff2e6-edff-461a-a0b7-a0a765a612f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    r_nino_precip =  xr.corr(\n",
    "        # TODO: Your code inside here\n",
    "    ).compute()\n",
    "\n",
    "r_nino_precip.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f258de8-05d0-42b9-9a23-d7191af7da45",
   "metadata": {},
   "source": [
    "# Reflections and next steps\n",
    "\n",
    "Before we part ways, write a sentence or two about the final plot you made above. Where does ENSO matter? Do you think this tells the whole story?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b6bd3-f004-4a39-a2a9-f21383261f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "has_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:35:26) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1968ed2f6a1384ed87e35b525e42af88639a205e7e4b8ed0bed37a2a35cd3ff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
